from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestRegressor

from tqdm import tqdm
import torch
import numpy as np
import pandas as pd
from scipy.sparse import hstack, vstack
from utils import one_hot_encode_batch, plot_confusion, load_dataset, load_negative_samples, load_swapped_positive_samples, save_encoded_data, load_encoded_data
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

from sklearn.metrics import recall_score, f1_score, precision_score, confusion_matrix
from scipy.sparse import hstack, vstack
import numpy as np
import pandas as pd

# from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

# Check if CUDA is available to attach, as using GPU will significantly speed process up


def train_random_forest(X_train, y_train, n_estimators = 100, max_depth = 30):
    # Following the seed tradition here?? lol https://www.reddit.com/r/datascience/comments/17kxd5s/data_folks_of_reddit_how_do_you_choose_a_random/
    # When n jobs is -1, utilizes all cores to train ASAP
    random_forest_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42, max_depth=max_depth, n_jobs=-1, min_samples_split=10)
    random_forest_model.fit(X_train, y_train)
    return random_forest_model


def compute_top1_f1_score(model, seq1_encoded, seq2_encoded, merged_df, val_idx):
    y_true = []
    y_pred = []

    with torch.no_grad():
        for i in tqdm(val_idx):
            seq1 = seq1_encoded[i]  # Get the seq1 encoding for the current index

            seq_2s_to_score_with_seq_1 = []
            true_labels = []

            scores_against_seq1 = []
            # Pair seq1 with all other seq2 (skip the same pair for the true match)
            for j in val_idx:
                seq2 = seq2_encoded[j]
                
                label = 1 if i == j else 0  # True match for i == j, else negative
                true_labels.append(label)

                # Concatenate seq1 and seq2 as in training phase
                pair_input = hstack([seq1, seq2])  # Combine seq1 and seq2 for each pair
                seq_2s_to_score_with_seq_1.append(pair_input)
            
            input_batch = vstack(seq_2s_to_score_with_seq_1)
            scores_against_seq1 = model.predict(input_batch)
            # Find top-1 prediction (index with the highest output value)
            top1_idx = np.argmax(scores_against_seq1)
            predicted_label = 1 if true_labels[top1_idx] == 1 else 0

            # Append true label (always 1, because seq1 has exactly one match)
            y_true.append(1)
            y_pred.append(predicted_label)

    # Compute precision, recall, and F1 score based on the true vs predicted labels
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"Top-1 Precision: {precision:.4f}")
    print(f"Top-1 Recall: {recall:.4f}")
    print(f"Top-1 F1 Score: {f1:.4f}")
    plot_confusion(y_pred, y_true )
    return {"precision": precision, "recall": recall, "f1": f1}


def main():
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
    print("LOADING BACARCH DATASET, ADDING NEGATIVE SAMPLES, AND SHUFFLING....")
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")    
    raw_dataframe = load_dataset('BacArch')
    raw_dataframe["pair_id"] = range(len(raw_dataframe))
    raw_dataframe["source"] = "original"

    merged_df = raw_dataframe.sample(frac=1, random_state=42)

    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
    print("PADDING AND ENCODING SEQ 1 and SEQ 2 COLS....")
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")    
    encoded_filename = 'encoded_sequences.pkl'
    loaded_encodings = None
    if loaded_encodings is None:
        seq1_df = merged_df[['Seq1']].copy()
        seq1_df.columns = ['MatchedSeqs']
        seq2_df = merged_df[['Seq2']].copy()
        seq2_df.columns = ['MatchedSeqs']

        seq1_encoded = one_hot_encode_batch(seq1_df, 1000)
        seq2_encoded = one_hot_encode_batch(seq2_df, 1000)

        save_encoded_data(seq1_encoded, seq2_encoded, encoded_filename)
    else:
        seq1_encoded, seq2_encoded = loaded_encodings

    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
    print("CONCATENATING COLS FOR INPUTS....")
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")   
    X = hstack([seq1_encoded, seq2_encoded]) 
    y = np.array(merged_df["Label"])

    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
    print("TRAINING AND EVALUATING NN WITH K-FOLD CROSS-VALIDATION....")
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")    
    k_folds = 5
    fold_metrics = []
    fold_losses = []

    k_fold = StratifiedKFold(n_splits = k_folds, shuffle=True, random_state=42)

    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X, y)):
        print(f"\nTraining fold {fold+1}/{k_folds}...")

        # Assign training and validation sets
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]


        # print(train_idx)
        # print(val_idx)

        flipped_pos_samples = []
        for i in train_idx:
            seq1 = seq1_encoded[i]
            seq2 = seq2_encoded[i]
            flipped_pos_samples.append(hstack([seq2, seq1]))

        # Create negative samples for the train set by pairing each Seq1 with all other Seq2
        neg_samples = []
        for i in train_idx:
            seq1 = seq1_encoded[i]
            for j in train_idx:
                # except itself
                if i != j:
                    seq2 = seq2_encoded[j]
                    # We know these are not similar as there is only 1 true match in the dataset
                    neg_samples.append(hstack([seq1, seq2]))

        X_neg = vstack(neg_samples)
        y_neg = np.zeros(X_neg.shape[0])
        # X_neg, y_neg = shuffle(X_neg, y_neg, random_state=42)
        # X_neg = X_neg[:250]
        # y_neg = y_neg[:250]

        X_pos_flipped = vstack(flipped_pos_samples)
        y_pos_flipped = np.ones(X_pos_flipped.shape[0])

        repeat_factor = 3
        X_pos_repeated = vstack([X_train[y_train == 1]] * repeat_factor)
        y_pos_repeated = np.ones(X_pos_repeated.shape[0])



        X_train = vstack([X_train, X_neg, X_pos_flipped, X_pos_repeated])
        y_train = np.concatenate([y_train, y_neg, y_pos_flipped, y_pos_repeated])

        X_train, y_train = shuffle(X_train, y_train, random_state=42)

        random_forest_model = train_random_forest(X_train, y_train)


        metrics = compute_top1_f1_score(random_forest_model, seq1_encoded, seq2_encoded, merged_df, val_idx)
        fold_metrics.append(metrics)
        

    # After cross-validation, calculate average metrics across all folds
    avg_precision = np.mean([metrics['precision'] for metrics in fold_metrics])
    avg_recall = np.mean([metrics['recall'] for metrics in fold_metrics])
    avg_f1 = np.mean([metrics['f1'] for metrics in fold_metrics])

    print(f"\nAverage Metrics across {k_folds} folds:")
    print(f"Precision: {avg_precision:.4f}")
    print(f"Recall: {avg_recall:.4f}")
    print(f"F1 Score: {avg_f1:.4f}")

    # Convert the fold metrics to a DataFrame
    metrics_df = pd.DataFrame(fold_metrics)
    date = datetime.now()
    # Save the fold metrics to a CSV file
    metrics_df.to_csv(f'rf_fold_metrics_{date}.csv', index=False)

if __name__ == "__main__":
    main()